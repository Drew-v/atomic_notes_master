Inputs are combined with the initial weights in a weighted sum and subjected to the activation funciton. each linear combination is propagated to the next layer


Each layer feeds next one with result of their computation, their internal representation of the data. Goes all the way through hidden layers to the output layer. 

if algorithm only fed forward results to output and stopped there, there would be no learning ito min the weights of the cost function. Enter [[backpropagation]]