Ada grad, and Adam learning rate optimization algorithm. Sometimes they have problems with early stopping. If problem occurs, use nestorov. 
![[CB1A1486-1D23-45D2-A509-5C7CC220A346.jpeg]]

What is dropout? Every training loop, randomly dropping nodes which randomly happen to have value of zero. Dropout is method of regularization, to prevent overfitting to training, and generalizes to new data better. It is now used right before the output layer. 